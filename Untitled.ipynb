{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Breakout-ram-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "def get_policy_model(env, lr=0.001, hidden_layer_neurons = 128):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidden_layer_neurons, input_shape=env.observation_space.shape, activation='relu'))\n",
    "    model.add(Dense(env.action_space.n, activation='softmax'))\n",
    "    ## Por que la categorical_crossentropy funciona ok?\n",
    "    model.compile(Adam(lr), loss=['categorical_crossentropy'])\n",
    "    return model\n",
    "    \n",
    "def get_action(obs, model):\n",
    "    p = model.predict([self.observation.reshape(1, self.nS)])\n",
    "    if eval is False:\n",
    "        action = np.random.choice(self.nA, p=p[0]) #np.nan_to_num(p[0])\n",
    "    else:\n",
    "        action = np.argmax(p[0])\n",
    "    action_one_hot = np.zeros(self.nA)\n",
    "    action_one_hot[action] = 1\n",
    "    return action, action_one_hot, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_11 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 17,028\n",
      "Trainable params: 17,028\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(128)])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_policy_model(env)\n",
    "model.summary()\n",
    "model.input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128,)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "prob = model.predict(obs.reshape(1, 128))\n",
    "action = np.random.choice(range(env.action_space.n), p=prob[0])\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.000000e+00, 1.000000e+00, 7.852875e-13, 0.000000e+00]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 CPU times: user 208 ms, sys: 11.1 ms, total: 219 ms\n",
      "Wall time: 139 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "obs = env.reset()\n",
    "done = False\n",
    "step = 0\n",
    "observations = []\n",
    "while not done:\n",
    "    prob = model.predict(obs.reshape(1, 128))\n",
    "    action = np.random.choice(range(env.action_space.n), p=prob[0])\n",
    "    action_one_hot = np.zeros(env.action_space.n)\n",
    "    action_one_hot[action] = 1\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    observations.append(obs)\n",
    "    print(f'{action}', end=' ')\n",
    "    \n",
    "    step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ale.lives': 4}, 1.0, 97\n",
      "{'ale.lives': 4}, 1.0, 148\n",
      "{'ale.lives': 0}, 0.0, 272CPU times: user 360 ms, sys: 27.6 ms, total: 387 ms\n",
      "Wall time: 257 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "obs = env.reset()\n",
    "done = False\n",
    "step = 0\n",
    "observations = []\n",
    "n_experience_episodes = 1\n",
    "exp_episodes = 0\n",
    "ts_count = 0\n",
    "while exp_episodes < n_experience_episodes:\n",
    "    prob = model.predict(obs.reshape(1, 128))\n",
    "    action = np.random.choice(range(env.action_space.n), p=prob[0])\n",
    "    action_one_hot = np.zeros(env.action_space.n)\n",
    "    action_one_hot[action] = 1\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    observations.append(obs)\n",
    "    print(f'\\r{info}, {reward}, {step}', end='')\n",
    "    ts_count+=1\n",
    "    if done:\n",
    "        exp_episodes+=1\n",
    "        env.reset()\n",
    "    if reward>0:\n",
    "        print()\n",
    "    step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00000e+00, 3.92487e-21, 1.00000e+00, 0.00000e+00], dtype=float32)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_experience_episodes(n_experience_episodes = 1, return_ts=False):\n",
    "    # Antes de llamar esta función hay que asegurarse de que el env esta reseteado\n",
    "    observations = []\n",
    "    actions = []\n",
    "    predictions = []\n",
    "    rewards = []\n",
    "    discounted_rewards = []\n",
    "    episodes_returns = []\n",
    "    episodes_lenghts = []\n",
    "    time_steps = []\n",
    "    exp_episodes = 0\n",
    "    ts_count = 0\n",
    "    # Juega n_experience_episodes episodios\n",
    "    observation = env.reset()\n",
    "    while exp_episodes < n_experience_episodes:\n",
    "        # Obtengo acción\n",
    "        prediction = model.predict(observation.reshape(1, 128))\n",
    "        action = np.random.choice(range(env.action_space.n), p=prediction[0])\n",
    "        action_one_hot = np.zeros(env.action_space.n)\n",
    "        action_one_hot[action] = 1\n",
    "\n",
    "        # Ejecuto acción\n",
    "        observation_, reward, done, info = env.step(action)\n",
    "        print(f'\\r{ts_count}, {exp_episodes}, {done}, {info}', end='')\n",
    "        # Guardo reward obtenido por acción\n",
    "#         self.reward.append(reward)\n",
    "\n",
    "        # Notar que se guarda la observación anterior\n",
    "        observations.append(observation)\n",
    "\n",
    "        actions.append(action_one_hot)\n",
    "        predictions.append(prediction.flatten())\n",
    "        rewards.append(reward)\n",
    "        observation = observation_\n",
    "        ts_count+=1\n",
    "        time_steps.append(ts_count)\n",
    "        if done:\n",
    "            print('entra')\n",
    "            exp_episodes += 1\n",
    "#             discounted_reward = self.get_discounted_rewards(self.reward)\n",
    "#             discounted_rewards = np.hstack([discounted_rewards, discounted_reward])\n",
    "#             ep_len = len(discounted_reward)\n",
    "#             episodes_lenghts.append(ep_len)\n",
    "#             episodes_returns = episodes_returns + [discounted_reward[0]]*ep_len\n",
    "            last_observation = observation_\n",
    "            env.reset()\n",
    "            ts_count = 0\n",
    "    if return_ts:\n",
    "        return np.array(observations), np.array(actions), np.array(predictions),  np.array(rewards),   last_observation, np.array(time_steps).reshape(-1, 1)\n",
    "    else:\n",
    "        return np.array(observations), np.array(actions), np.array(predictions),  np.array(rewards),  last_observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159, 0, True, {'ale.lives': 0}entra\n",
      "CPU times: user 233 ms, sys: 11.6 ms, total: 244 ms\n",
      "Wall time: 162 ms\n"
     ]
    }
   ],
   "source": [
    "%time obs, actions, preds, rewards, last_obs = get_experience_episodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(160, 128)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
