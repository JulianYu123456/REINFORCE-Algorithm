{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym  #requires OpenAI gym installed\n",
    "env = gym.envs.make(\"MountainCarContinuous-v0\") \n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "input_dims = 2\n",
    "state_placeholder = tf.placeholder(tf.float32, [None, input_dims]) \n",
    "\n",
    "def value_function(state):\n",
    "    n_hidden1 = 400  \n",
    "    n_hidden2 = 400\n",
    "    n_outputs = 1\n",
    "    \n",
    "    with tf.variable_scope(\"value_network\"):\n",
    "        init_xavier = tf.contrib.layers.xavier_initializer()\n",
    "        \n",
    "        hidden1 = tf.layers.dense(state, n_hidden1, tf.nn.elu, init_xavier)\n",
    "        hidden2 = tf.layers.dense(hidden1, n_hidden2, tf.nn.elu, init_xavier) \n",
    "        V = tf.layers.dense(hidden2, n_outputs, None, init_xavier)\n",
    "    return V\n",
    "\n",
    "\n",
    "def policy_network(state):\n",
    "    n_hidden1 = 40\n",
    "    n_hidden2 = 40\n",
    "    n_outputs = 1\n",
    "    \n",
    "    with tf.variable_scope(\"policy_network\"):\n",
    "        init_xavier = tf.contrib.layers.xavier_initializer()\n",
    "        \n",
    "        hidden1 = tf.layers.dense(state, n_hidden1, tf.nn.elu, init_xavier)\n",
    "        hidden2 = tf.layers.dense(hidden1, n_hidden2, tf.nn.elu, init_xavier)\n",
    "        mu = tf.layers.dense(hidden2, n_outputs, None, init_xavier)\n",
    "        sigma = tf.layers.dense(hidden2, n_outputs, None, init_xavier)\n",
    "        sigma = tf.nn.softplus(sigma) + 1e-5\n",
    "        norm_dist = tf.contrib.distributions.Normal(mu, sigma)\n",
    "        action_tf_var = tf.squeeze(norm_dist.sample(1), axis=0)\n",
    "        action_tf_var = tf.clip_by_value(\n",
    "            action_tf_var, env.action_space.low[0], \n",
    "            env.action_space.high[0])\n",
    "    return action_tf_var, norm_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "#sample from state space for state normalization\n",
    "import sklearn\n",
    "import sklearn.preprocessing\n",
    "                                    \n",
    "state_space_samples = np.array(\n",
    "    [env.observation_space.sample() for x in range(10000)])\n",
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "scaler.fit(state_space_samples)\n",
    "\n",
    "#function to normalize states\n",
    "def scale_state(state):                 #requires input shape=(2,)\n",
    "    scaled = scaler.transform([state])\n",
    "    return scaled                       #returns shape =(1,2)   \n",
    "###################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lr_actor = 0.00002  #set learning rates\n",
    "lr_critic = 0.001\n",
    "\n",
    "# define required placeholders\n",
    "action_placeholder = tf.placeholder(tf.float32)\n",
    "delta_placeholder = tf.placeholder(tf.float32)\n",
    "target_placeholder = tf.placeholder(tf.float32)\n",
    "\n",
    "action_tf_var, norm_dist = policy_network(state_placeholder)\n",
    "V = value_function(state_placeholder)\n",
    "\n",
    "# define actor (policy) loss function\n",
    "loss_actor = -tf.log(norm_dist.prob(action_placeholder) + 1e-5) * delta_placeholder\n",
    "training_op_actor = tf.train.AdamOptimizer(\n",
    "    lr_actor, name='actor_optimizer').minimize(loss_actor)\n",
    "\n",
    "# define critic (state-value) loss function\n",
    "loss_critic = tf.reduce_mean(tf.squared_difference(\n",
    "                             tf.squeeze(V), target_placeholder))\n",
    "training_op_critic = tf.train.AdamOptimizer(\n",
    "        lr_critic, name='critic_optimizer').minimize(loss_critic)\n",
    "################################################################\n",
    "#Training loop\n",
    "gamma = 0.99        #discount factor\n",
    "num_episodes = 300\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    episode_history = []\n",
    "    for episode in range(num_episodes):\n",
    "        #receive initial state from E\n",
    "        state = env.reset()   # state.shape -> (2,)\n",
    "        reward_total = 0 \n",
    "        steps = 0\n",
    "        done = False\n",
    "        while (not done):\n",
    "                \n",
    "            #Sample action according to current policy\n",
    "            #action.shape = (1,1)\n",
    "            action  = sess.run(action_tf_var, feed_dict={\n",
    "                          state_placeholder: scale_state(state)})\n",
    "            #Execute action and observe reward & next state from E\n",
    "            # next_state shape=(2,)    \n",
    "            #env.step() requires input shape = (1,)\n",
    "            next_state, reward, done, _ = env.step(\n",
    "                                    np.squeeze(action, axis=0)) \n",
    "            steps +=1\n",
    "            reward_total += reward\n",
    "            #V_of_next_state.shape=(1,1)\n",
    "            V_of_next_state = sess.run(V, feed_dict = \n",
    "                    {state_placeholder: scale_state(next_state)})  \n",
    "            #Set TD Target\n",
    "            #target = r + gamma * V(next_state)     \n",
    "            target = reward + gamma * np.squeeze(V_of_next_state) \n",
    "            \n",
    "            # td_error = target - V(s)\n",
    "            #needed to feed delta_placeholder in actor training\n",
    "            td_error = target - np.squeeze(sess.run(V, feed_dict = \n",
    "                        {state_placeholder: scale_state(state)})) \n",
    "            \n",
    "            #Update actor by minimizing loss (Actor training)\n",
    "            _, loss_actor_val  = sess.run(\n",
    "                [training_op_actor, loss_actor], \n",
    "                feed_dict={action_placeholder: np.squeeze(action), \n",
    "                state_placeholder: scale_state(state), \n",
    "                delta_placeholder: td_error})\n",
    "            #Update critic by minimizinf loss  (Critic training)\n",
    "            _, loss_critic_val  = sess.run(\n",
    "                [training_op_critic, loss_critic], \n",
    "                feed_dict={state_placeholder: scale_state(state), \n",
    "                target_placeholder: target})\n",
    "            \n",
    "            state = next_state\n",
    "            #end while\n",
    "        episode_history.append(reward_total)\n",
    "        print(\"Episode: {}, Number of Steps : {}, Cumulative reward: {:0.2f}\".format(\n",
    "            episode, steps, reward_total))\n",
    "        \n",
    "        if np.mean(episode_history[-100:]) > 90 and len(episode_history) >= 101:\n",
    "            print(\"****************Solved***************\")\n",
    "            print(\"Mean cumulative reward over 100 episodes:{:0.2f}\" .format(\n",
    "                np.mean(episode_history[-100:])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
